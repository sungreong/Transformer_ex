{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('test': conda)"
  },
  "interpreter": {
   "hash": "d3773b654528d79880d3e2b33a682f0f55f4d8370c8ead39985baabf4eb4f1db"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Reference\n",
    "\n",
    "[visualizeda attention mechanism](https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853)\n",
    "\n",
    "[transformer linebyline example](https://paul-hyun.github.io/transformer-01/)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "def data_process(raw_text_iter):\n",
    "  data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "  return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([   9, 3849, 3869,  ..., 2442, 4810,    3])"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "source": [
    "* narrow 역할\n",
    "    * 깔끔하게 나누어 떨어지지 않는 추가적인 부분(나머지들) 은 잘라냅니다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    # 데이터셋을 bsz 파트들로 나눕니다.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # 깔끔하게 나누어 떨어지지 않는 추가적인 부분(나머지들) 은 잘라냅니다.\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # 데이터에 대하여 bsz 배치들로 동등하게 나눕니다.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([102499, 20])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "source": [
    "* get_batch() \n",
    "    * 함수는 트랜스포머 모델을 위한 입력과 타겟 시퀀스를 생성합니다. \n",
    "    * 이 함수는 소스 데이터를 bptt 길이를 가진 덩어리로 세분화 합니다. \n",
    "    * 언어 모델링 과제를 위해서, 모델은 다음 단어인 Target 이 필요 합니다. \n",
    "    * 예를 들어, bptt 의 값이 2 라면, 우리는 i = 0 일 때 다음의 2 개의 변수(Variable) 를 얻을 수 있습니다\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* BPTT\n",
    "    * BPTT를 통해 RNN의 가중치 행렬의 미분을 계산해보면 아래와 같이 최종적으로 미분의 곱으로 이루어진 항이 계산된다\n",
    "    * 시퀀스 길이가 길어지는 경우 BPTT가 불안정해지므로 길이를 끊는 것이 필요하다. 이 방법을 Truncated BPTT라고 부른다.\n",
    "    "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len] # .reshape(-1)\n",
    "    return data, target\n",
    "\n",
    "bptt_list  = [i for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt))]\n",
    "data, targets =get_batch(train_data , bptt_list[0])"
   ]
  },
  {
   "source": [
    "# Transpose\n",
    "\n",
    "(bptt, batch) -> (batch,bptt)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import math \n",
    "import numpy as np\n",
    "ntokens = len(vocab) # the size of vocabulary\n",
    "emsize = 512 # embedding dimension\n",
    "ninp = emsize\n",
    "encoder = nn.Embedding(ntokens, ninp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = encoder(data) * math.sqrt(ninp) ## question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([20, 35]), torch.Size([20, 35, 512]))"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "data.shape , src.shape"
   ]
  },
  {
   "source": [
    "# Scale-Dot Product Attention\n",
    "\n",
    "* Q와 K 사이를 내적하여 어텐션을 softmax를 통해 구하고, 그 후에 V를 내적하여 중요한 부분(Attention)을 더 살린다는 의미 내포\n",
    "\n",
    "## mask\n",
    "* self attention에서는 time sequence와 같이 적용해야하므로 필수\n",
    "\n",
    "## return \n",
    "* context\n",
    "    * softmax와 내적하여 context vector  rntjd \n",
    "* Attention"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_mask = generate_square_subsequent_mask(bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "  def __init__(self,d_k):\n",
    "    super(ScaledDotProductAttention, self).__init__()\n",
    "    self.d_k = d_k\n",
    "\n",
    "  def forward(self, Q, K, V, attn_mask=None):\n",
    "    scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(self.d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "    if attn_mask is not None:\n",
    "      scores.masked_fill_(attn_mask, -1e9)\n",
    "    attn = nn.Softmax(dim=-1)(scores)\n",
    "    context = torch.matmul(attn, V)\n",
    "    return context, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([20, 512, 35])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, dim_in: int, dim_k: int, dim_v: int):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(dim_in, dim_k)\n",
    "        self.k = nn.Linear(dim_in, dim_k)\n",
    "        self.v = nn.Linear(dim_in, dim_v)\n",
    "        self.scaledot = ScaledDotProductAttention(dim_k)\n",
    "\n",
    "    def forward(self,\n",
    "    query: Tensor, key: Tensor, value: Tensor,attn_mask:Tensor=None) -> Tensor:\n",
    "        return self.scaledot(self.q(query), self.k(key), self.v(value),attn_mask)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "512 8 64 64\n"
     ]
    }
   ],
   "source": [
    "dim_model = ninp\n",
    "num_heads = 8\n",
    "dim_k = dim_v = dim_model // num_heads\n",
    "print(dim_model, num_heads , dim_k , dim_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([20, 35, 35]), torch.Size([20, 35, 512]))"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "attention_value = ScaledDotProductAttention(dim_k)(src,src,src)\n",
    "attention_value[1].shape , attention_value[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([20, 35, 64])"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "attention = AttentionHead(dim_model , dim_k , dim_v)\n",
    "attention(src,src,src)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([20, 35, 512]), torch.Size([35, 35]))"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "src_mask = generate_square_subsequent_mask(src.shape[1])\n",
    "src.shape , src_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([20, 35, 64])"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "attention1head = AttentionHead(dim_model , dim_k , dim_v)\n",
    "attention1head(src,src,src,src_mask)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, dim_in: int, dim_k: int, dim_v: int):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(dim_in, dim_k, dim_v) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.linear = nn.Linear(num_heads * dim_v, dim_in)\n",
    "        print(len(self.heads),num_heads * dim_v, dim_in)\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor,attn_mask:Tensor=None) -> Tensor:\n",
    "        attn_cat = torch.cat([h(query, key, value,attn_mask)[0] for h in self.heads], dim=-1)\n",
    "        print(attn_cat.shape)\n",
    "        return self.linear(attn_cat)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "num_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "64\n8 512 512\ntorch.Size([20, 35, 512])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[-1.0696e+01,  4.0669e+00,  4.1515e+00,  ..., -8.7903e+00,\n",
       "          -8.7064e+00, -9.1512e-01],\n",
       "         [ 1.0199e+01, -9.8666e+00, -7.7445e+00,  ..., -1.1177e+01,\n",
       "           2.1795e+00, -4.6826e+00],\n",
       "         [-7.1328e+00,  4.3087e-02, -1.6991e+00,  ..., -2.2577e+00,\n",
       "          -1.2382e-01, -1.0623e+01],\n",
       "         ...,\n",
       "         [ 1.1241e+01, -3.3353e-01, -1.1624e+00,  ..., -1.2642e+01,\n",
       "          -3.4864e+00, -1.3560e+00],\n",
       "         [ 9.1558e+00, -1.0391e+01, -1.4705e+01,  ...,  5.9991e+00,\n",
       "          -9.5669e+00, -4.5261e+00],\n",
       "         [ 4.5109e+00, -6.5855e+00,  5.5173e+00,  ..., -2.8333e+00,\n",
       "          -8.1005e-01, -1.4307e+01]],\n",
       "\n",
       "        [[-1.0836e+01, -1.9000e+00,  6.5398e+00,  ..., -5.4715e+00,\n",
       "          -3.5084e+00,  3.6073e+00],\n",
       "         [-2.1476e-01,  5.7531e+00, -1.0472e+00,  ...,  7.5718e+00,\n",
       "           1.3334e+00,  5.6058e+00],\n",
       "         [ 1.6913e+00,  5.1038e+00, -1.0241e+01,  ...,  1.3705e+01,\n",
       "          -1.8139e+00,  5.1536e+00],\n",
       "         ...,\n",
       "         [ 2.8922e+00,  3.1127e+00, -3.6887e-01,  ..., -1.2806e+01,\n",
       "          -8.5007e+00, -9.9811e+00],\n",
       "         [-9.1002e+00,  4.5544e+00,  6.0630e+00,  ...,  5.0554e-01,\n",
       "           4.7912e-01, -1.2844e+00],\n",
       "         [ 6.7883e+00, -1.2324e+01,  7.2189e+00,  ..., -2.1228e+00,\n",
       "          -5.5481e-01,  6.1755e-01]],\n",
       "\n",
       "        [[ 5.9783e+00, -1.8534e+01,  4.3710e+00,  ...,  1.3309e+01,\n",
       "           9.4840e+00,  1.1908e+01],\n",
       "         [-1.1804e+01, -4.4200e-01,  2.4246e+00,  ..., -5.8237e+00,\n",
       "           7.0942e-01, -1.2505e+00],\n",
       "         [-1.9928e+00,  4.9086e+00,  6.1400e+00,  ..., -8.7066e+00,\n",
       "          -1.7384e+00, -8.5696e+00],\n",
       "         ...,\n",
       "         [-6.8533e+00, -3.1474e+00,  7.0626e+00,  ...,  5.9348e+00,\n",
       "           8.7762e+00,  2.8778e+00],\n",
       "         [-9.9884e+00, -8.6352e+00, -7.3364e+00,  ...,  6.3155e+00,\n",
       "          -2.1425e+01, -8.0206e+00],\n",
       "         [-1.0650e+01,  2.8729e+00, -1.4231e+00,  ..., -1.0638e+01,\n",
       "          -5.3688e+00, -3.3657e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-2.8000e+00, -5.4897e+00, -3.1758e+00,  ..., -1.9150e+01,\n",
       "           7.0665e+00,  5.6375e+00],\n",
       "         [ 4.9858e+00, -3.6985e+00, -4.2617e+00,  ..., -3.8755e+00,\n",
       "          -4.9024e+00,  9.6435e+00],\n",
       "         [-3.5572e+00, -5.3204e+00, -8.0359e+00,  ..., -3.0324e+00,\n",
       "           2.7029e+00, -1.2697e+01],\n",
       "         ...,\n",
       "         [ 5.1491e+00,  5.3254e+00, -9.4560e-01,  ...,  7.4133e+00,\n",
       "           9.0969e+00,  1.2642e+00],\n",
       "         [ 2.0023e+00, -1.0135e+01,  6.1144e+00,  ..., -1.4266e+01,\n",
       "          -8.8024e+00, -1.3182e+01],\n",
       "         [ 2.9334e+00, -3.3189e+00, -9.1141e+00,  ...,  2.3838e+00,\n",
       "          -5.9407e+00,  2.1006e+00]],\n",
       "\n",
       "        [[-1.1249e+01, -7.3372e+00,  2.0187e-02,  ..., -2.1180e+00,\n",
       "           9.9854e+00, -1.5106e+01],\n",
       "         [-1.0637e+01,  4.2845e+00,  3.0386e+00,  ..., -7.2656e+00,\n",
       "           1.0116e+01, -1.2580e+00],\n",
       "         [ 6.5254e+00, -8.8546e+00, -5.7656e+00,  ..., -1.7409e+01,\n",
       "          -1.7316e+01,  1.1076e+01],\n",
       "         ...,\n",
       "         [ 4.3171e+00, -1.6520e+01, -1.7292e+01,  ...,  4.6013e+00,\n",
       "           1.4658e+01,  3.9919e+00],\n",
       "         [-1.0332e+01, -1.3291e+01, -4.5721e+00,  ..., -4.7670e+00,\n",
       "           9.9800e-01,  9.6342e-01],\n",
       "         [ 2.8438e+00, -1.0377e+01,  4.2488e-01,  ..., -1.3094e+01,\n",
       "          -6.6020e+00, -1.1528e+01]],\n",
       "\n",
       "        [[ 1.2308e+00, -4.2788e-01,  7.4196e+00,  ..., -2.8239e+00,\n",
       "           1.2295e+00, -4.2518e+00],\n",
       "         [-6.6604e+00, -1.1245e+01, -4.5603e+00,  ..., -3.2522e+00,\n",
       "           2.9650e+00,  7.6144e+00],\n",
       "         [-5.4309e+00,  1.9751e+00,  7.6113e+00,  ...,  2.6780e+00,\n",
       "           1.5792e+01, -2.6444e+00],\n",
       "         ...,\n",
       "         [ 9.6230e+00, -2.9481e+00, -1.8944e+01,  ...,  4.3809e-01,\n",
       "           3.8437e+00, -4.0883e+00],\n",
       "         [-3.2814e+00, -2.2571e+00,  1.5365e+00,  ..., -1.2810e+01,\n",
       "          -6.4281e+00, -1.5131e+01],\n",
       "         [ 9.5586e-01, -2.7276e-01, -7.8847e+00,  ...,  4.1862e+00,\n",
       "          -7.6452e+00,  6.7578e-01]]], grad_fn=<AddBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "print(dim_v)\n",
    "multiheadattention = MultiHeadAttention(num_heads,dim_model,dim_k,dim_v)\n",
    "src_mask = generate_square_subsequent_mask(src.shape[1])\n",
    "multiheadattention(src,src,src,src_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = src\n",
    "K = src\n",
    "V = src\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([35, 35]), torch.Size([20, 35, 512]), 35)"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "src_mask.shape , Q.shape ,  K.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src_mask2 = src_mask.repeat(20,1,1)\n",
    "# src_mask2.eq(0).unsqueeze(1).expand(Q.size(0), Q.size(1), K.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([20, 35, 512])\ntorch.Size([20, 35, 8, 64])\ntorch.Size([20, 8, 35, 64])\n"
     ]
    }
   ],
   "source": [
    "dim_in = dim_model\n",
    "W_Q = nn.Linear(dim_in, num_heads * dim_k)\n",
    "W_K = nn.Linear(dim_in, num_heads * dim_k)\n",
    "W_V = nn.Linear(dim_in, num_heads * dim_k)\n",
    "\n",
    "# (bs, n_seq, n_head * d_head)\n",
    "q_s = W_Q(Q)\n",
    "print(q_s.size())\n",
    "# (bs, n_seq, n_head, d_head)\n",
    "q_s = q_s.view(batch_size, -1, num_heads, dim_k)\n",
    "print(q_s.size())\n",
    "# (bs, n_head, n_seq, d_head)\n",
    "q_s = q_s.transpose(1,2)\n",
    "print(q_s.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([20, 8, 35, 64]) torch.Size([20, 8, 35, 64]) torch.Size([20, 8, 35, 64])\n"
     ]
    }
   ],
   "source": [
    "# (bs, n_head, n_seq, d_head)\n",
    "q_s = W_Q(Q).view(batch_size, -1, num_heads, dim_k).transpose(1,2)\n",
    "# (bs, n_head, n_seq, d_head)\n",
    "k_s = W_K(K).view(batch_size, -1, num_heads, dim_k).transpose(1,2)\n",
    "# (bs, n_head, n_seq, d_head)\n",
    "v_s = W_V(V).view(batch_size, -1, num_heads, dim_k).transpose(1,2)\n",
    "print(q_s.size(), k_s.size(), v_s.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(attn_mask.size())\n",
    "# attn_mask = attn_mask.unsqueeze(1).repeat(1, n_head, 1, 1)\n",
    "# print(attn_mask.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([20, 8, 35, 64])\ntorch.Size([20, 8, 35, 35])\n"
     ]
    }
   ],
   "source": [
    "scaled_dot_attn = ScaledDotProductAttention(dim_k)\n",
    "context, attn_prob = scaled_dot_attn(q_s, k_s, v_s, attn_mask= None)\n",
    "print(context.size())\n",
    "print(attn_prob.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([20, 35, 512])\n"
     ]
    }
   ],
   "source": [
    "context = torch.matmul(attn_prob, V)\n",
    "print(context.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([20, 35, 512])\n"
     ]
    }
   ],
   "source": [
    "context = context.transpose(1, 2).contiguous().view(batch_size, -1, num_heads * dim_k)\n",
    "print(context.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([20, 35, 512])\n"
     ]
    }
   ],
   "source": [
    "linear = nn.Linear(num_heads * dim_k, dim_in)\n",
    "# (bs, n_seq, d_hidn)\n",
    "output = linear(context)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" multi head attention \"\"\"\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_in, num_heads, dim_k):\n",
    "        super().__init__()\n",
    "        self.dim_in = dim_in\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_k = dim_k\n",
    "\n",
    "        self.W_Q = nn.Linear(dim_in, num_heads * dim_k)\n",
    "        self.W_K = nn.Linear(dim_in, num_heads * dim_k)\n",
    "        self.W_V = nn.Linear(dim_in, num_heads * dim_k)\n",
    "        self.scaled_dot_attn = ScaledDotProductAttention(dim_k)\n",
    "        self.linear = nn.Linear(num_heads * dim_k, dim_in)\n",
    "    \n",
    "    def forward(self, Q, K, V, attn_mask=None):\n",
    "        batch_size = Q.size(0)\n",
    "        # (bs, n_head, n_q_seq, d_head)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, self.num_heads, self.dim_k).transpose(1,2)\n",
    "        # (bs, n_head, n_k_seq, d_head)\n",
    "        k_s = self.W_K(K).view(batch_size, -1, self.num_heads, self.dim_k).transpose(1,2)\n",
    "        # (bs, n_head, n_v_seq, d_head)\n",
    "        v_s = self.W_V(V).view(batch_size, -1, self.num_heads, self.dim_k).transpose(1,2)\n",
    "\n",
    "        # (bs, n_head, n_q_seq, n_k_seq) TODO:\n",
    "        if attn_mask is not None :\n",
    "            attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_head, 1, 1)\n",
    "\n",
    "        # (bs, n_head, n_q_seq, d_head), (bs, n_head, n_q_seq, n_k_seq)\n",
    "        context, attn_prob = self.scaled_dot_attn(q_s, k_s, v_s, attn_mask=attn_mask)\n",
    "        # (bs, n_head, n_q_seq, h_head * d_head)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.dim_k)\n",
    "        # (bs, n_head, n_q_seq, e_embd)\n",
    "        output = self.linear(context)\n",
    "        # (bs, n_q_seq, d_hidn), (bs, n_head, n_q_seq, n_k_seq)\n",
    "        return output, attn_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([20, 35, 512])"
      ]
     },
     "metadata": {},
     "execution_count": 84
    }
   ],
   "source": [
    "multiatten = MultiHeadAttention(dim_in=dim_in,num_heads=num_heads,dim_k=dim_k)\n",
    "multiatten(Q,K,V,None)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([20, 8, 35, 35])"
      ]
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "source": [
    "multiatten(Q,K,V,None)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}